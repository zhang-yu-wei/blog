<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/1f1ff.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/iconz_32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/iconz_16.png">
  <link rel="mask-icon" href="/images/1f1ff.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Picture from https:&#x2F;&#x2F;www.entefy.com&#x2F;blog&#x2F;post&#x2F;584&#x2F;entefy%E2%80%99s-quick-introduction-to-multimodal-ai-video A Summary for Vision-Language (VL) Transformers In this post, I present the VL Transformer">
<meta property="og:type" content="article">
<meta property="og:title" content="A Summary for Vision-Language Transformers">
<meta property="og:url" content="http://yoursite.com/2021/12/24/VLP/index.html">
<meta property="og:site_name" content="Blogs from Yuwei Zhang (张峪玮)">
<meta property="og:description" content="Picture from https:&#x2F;&#x2F;www.entefy.com&#x2F;blog&#x2F;post&#x2F;584&#x2F;entefy%E2%80%99s-quick-introduction-to-multimodal-ai-video A Summary for Vision-Language (VL) Transformers In this post, I present the VL Transformer">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/images/multimodal.jpeg">
<meta property="og:image" content="http://yoursite.com/images/modality_fusing.png">
<meta property="og:image" content="http://yoursite.com/images/vilbert.png">
<meta property="og:image" content="http://yoursite.com/images/uniter.png">
<meta property="og:image" content="http://yoursite.com/images/lack_global_info.png">
<meta property="og:image" content="http://yoursite.com/images/vilt.png">
<meta property="og:image" content="http://yoursite.com/images/mdetr.png">
<meta property="og:image" content="http://yoursite.com/images/u_visualbert.png">
<meta property="og:image" content="http://yoursite.com/images/unimo.png">
<meta property="og:image" content="http://yoursite.com/images/prompt.png">
<meta property="og:image" content="http://yoursite.com/images/frozen.png">
<meta property="og:image" content="http://yoursite.com/images/vl_t5.png">
<meta property="og:image" content="http://yoursite.com/images/cpt.png">
<meta property="og:image" content="http://yoursite.com/images/generated.png">
<meta property="article:published_time" content="2021-12-25T06:46:50.000Z">
<meta property="article:modified_time" content="2021-12-28T06:16:39.121Z">
<meta property="article:author" content="Yuwei Zhang (张峪玮)">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/images/multimodal.jpeg">

<link rel="canonical" href="http://yoursite.com/2021/12/24/VLP/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>A Summary for Vision-Language Transformers | Blogs from Yuwei Zhang (张峪玮)</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Blogs from Yuwei Zhang (张峪玮)</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Exploration of efficient, effective and explainable deep learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/12/24/VLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/maomi.jpg">
      <meta itemprop="name" content="Yuwei Zhang (张峪玮)">
      <meta itemprop="description" content="I am a M.S. student in UCSD. I received B.S. in Physics from Nankai University in China. I currently work on NLP/CV/ML. This blog will record my readings and thoughts from research.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blogs from Yuwei Zhang (张峪玮)">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A Summary for Vision-Language Transformers
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-24 22:46:50" itemprop="dateCreated datePublished" datetime="2021-12-24T22:46:50-08:00">2021-12-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-27 22:16:39" itemprop="dateModified" datetime="2021-12-27T22:16:39-08:00">2021-12-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/../images/multimodal.jpeg" /></p>
<p><sub><sup>Picture from https://www.entefy.com/blog/post/584/entefy%E2%80%99s-quick-introduction-to-multimodal-ai-video</sup></sub></p>
<h1 id="a-summary-for-vision-language-vl-transformers">A Summary for Vision-Language (VL) Transformers</h1>
<p>In this post, I present the VL Transformers from its basic formulation to the recent progress that aims at improving its flexibility, efficiency and performance.</p>
<a id="more"></a>
<h3 id="what-is-it">What is it?</h3>
<p>Transformers are becoming the most popular architecture in both NLP and CV. For instance, on NLP there are <a target="_blank" rel="noopener" href="https://aclanthology.org/N19-1423/">BERT</a> &amp; <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">GPT-3</a> for language understanding, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.10683.pdf">T5</a> for language generation and <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.07291">XLM</a> for multi-lingual representations, on CV there are <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">ViT</a> &amp; <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.14030">Swin-T</a> for general image representation and <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.12872">DETR</a> for object detection. They usually require the inputs as a sequence of tokens <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.69ex" xmlns="http://www.w3.org/2000/svg" width="13.079ex" height="2.735ex" role="img" focusable="false" viewBox="0 -903.7 5780.8 1208.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path><path id="MJX-1-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-1-TEX-D-211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><use data-c="7B" xlink:href="#MJX-1-TEX-N-7B"></use></g><g data-mml-node="msub" transform="translate(500,0)"><g data-mml-node="mi"><use data-c="1D44B" xlink:href="#MJX-1-TEX-I-1D44B"></use></g><g data-mml-node="mi" transform="translate(861,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(1932.7,0)"><use data-c="2208" xlink:href="#MJX-1-TEX-N-2208"></use></g><g data-mml-node="msup" transform="translate(2877.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="211D" xlink:href="#MJX-1-TEX-D-211D"></use></g></g><g data-mml-node="TeXAtom" transform="translate(755,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D451" xlink:href="#MJX-1-TEX-I-1D451"></use></g></g></g><g data-mml-node="msubsup" transform="translate(4050.2,0)"><g data-mml-node="mo"><use data-c="7D" xlink:href="#MJX-1-TEX-N-7D"></use></g><g data-mml-node="mi" transform="translate(533,413) scale(0.707)"><use data-c="1D447" xlink:href="#MJX-1-TEX-I-1D447"></use></g><g data-mml-node="TeXAtom" transform="translate(533,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g></g></svg></mjx-container>. Specifically, this could be a set of image patches, CNN features or word embeddings. A class token <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.16ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1396.5 599.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-1-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-1-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="TeXAtom" transform="translate(498,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D450" xlink:href="#MJX-1-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(433,0)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(731,0)"><use data-c="1D460" xlink:href="#MJX-1-TEX-I-1D460"></use></g></g></g></g></g></svg></mjx-container> is prepended at front to represent the overall feature of the sequence and position embedding <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.699ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 751 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D443" xlink:href="#MJX-1-TEX-I-1D443"></use></g></g></g></svg></mjx-container> is added to indicate the position of tokens in the sequence. Thus the input sequence can be written as:</p>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="33.25ex" height="2.565ex" role="img" focusable="false" viewBox="0 -883.9 14696.4 1133.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path id="MJX-1-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-1-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-I-1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path id="MJX-1-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mn" transform="translate(498,413) scale(0.707)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mo" transform="translate(1179.3,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2235.1,0)"><use data-c="5B" xlink:href="#MJX-1-TEX-N-5B"></use></g><g data-mml-node="msub" transform="translate(2513.1,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="TeXAtom" transform="translate(498,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D450" xlink:href="#MJX-1-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(433,0)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(731,0)"><use data-c="1D460" xlink:href="#MJX-1-TEX-I-1D460"></use></g></g></g><g data-mml-node="mstyle" transform="translate(3909.6,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="mi" transform="translate(4909.6,0)"><use data-c="1D438" xlink:href="#MJX-1-TEX-I-1D438"></use></g><g data-mml-node="msub" transform="translate(5673.6,0)"><g data-mml-node="mi"><use data-c="1D44B" xlink:href="#MJX-1-TEX-I-1D44B"></use></g><g data-mml-node="mn" transform="translate(861,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mstyle" transform="translate(6938.2,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="mo" transform="translate(7938.2,0)"><use data-c="2E" xlink:href="#MJX-1-TEX-N-2E"></use></g><g data-mml-node="mo" transform="translate(8382.9,0)"><use data-c="2E" xlink:href="#MJX-1-TEX-N-2E"></use></g><g data-mml-node="mo" transform="translate(8827.5,0)"><use data-c="2E" xlink:href="#MJX-1-TEX-N-2E"></use></g><g data-mml-node="mstyle" transform="translate(9105.5,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="mi" transform="translate(10272.2,0)"><use data-c="1D438" xlink:href="#MJX-1-TEX-I-1D438"></use></g><g data-mml-node="msub" transform="translate(11036.2,0)"><g data-mml-node="mi"><use data-c="1D44B" xlink:href="#MJX-1-TEX-I-1D44B"></use></g><g data-mml-node="mi" transform="translate(861,-150) scale(0.707)"><use data-c="1D447" xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mo" transform="translate(12445,0)"><use data-c="5D" xlink:href="#MJX-1-TEX-N-5D"></use></g><g data-mml-node="mo" transform="translate(12945.2,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(13945.4,0)"><use data-c="1D443" xlink:href="#MJX-1-TEX-I-1D443"></use></g></g></g></svg></mjx-container>
<p>where <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.729ex" height="1.538ex" role="img" focusable="false" viewBox="0 -680 764 680" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D438" xlink:href="#MJX-1-TEX-I-1D438"></use></g></g></g></svg></mjx-container> is the projection operator to align the original data to input token. And then layers of attention mechanism is applied to find connections between tokens.</p>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="49.445ex" height="2.61ex" role="img" focusable="false" viewBox="0 -903.7 21854.7 1153.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z"></path><path id="MJX-1-TEX-N-53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z"></path><path id="MJX-1-TEX-N-41" d="M255 0Q240 3 140 3Q48 3 39 0H32V46H47Q119 49 139 88Q140 91 192 245T295 553T348 708Q351 716 366 716H376Q396 715 400 709Q402 707 508 390L617 67Q624 54 636 51T687 46H717V0H708Q699 3 581 3Q458 3 437 0H427V46H440Q510 46 510 64Q510 66 486 138L462 209H229L209 150Q189 91 189 85Q189 72 209 59T259 46H264V0H255ZM447 255L345 557L244 256Q244 255 345 255H447Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z"></path><path id="MJX-1-TEX-N-4E" d="M42 46Q74 48 94 56T118 69T128 86V634H124Q114 637 52 637H25V683H232L235 680Q237 679 322 554T493 303L578 178V598Q572 608 568 613T544 627T492 637H475V683H483Q498 680 600 680Q706 680 715 683H724V637H707Q634 633 622 598L621 302V6L614 0H600Q585 0 582 3T481 150T282 443T171 605V345L172 86Q183 50 257 46H274V0H265Q250 3 150 3Q48 3 33 0H25V46H42Z"></path><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mi" transform="translate(523,413) scale(0.707)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g></g><g data-mml-node="mo" transform="translate(1061.5,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mtext" transform="translate(2117.3,0)"><use data-c="4D" xlink:href="#MJX-1-TEX-N-4D"></use><use data-c="53" xlink:href="#MJX-1-TEX-N-53" transform="translate(917,0)"></use><use data-c="41" xlink:href="#MJX-1-TEX-N-41" transform="translate(1473,0)"></use></g><g data-mml-node="mo" transform="translate(4340.3,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mtext" transform="translate(4729.3,0)"><use data-c="4C" xlink:href="#MJX-1-TEX-N-4C"></use><use data-c="4E" xlink:href="#MJX-1-TEX-N-4E" transform="translate(625,0)"></use></g><g data-mml-node="mo" transform="translate(6104.3,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(6493.3,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="TeXAtom" transform="translate(498,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g><g data-mml-node="mo" transform="translate(298,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1076,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(8155.7,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(8544.7,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(9155.9,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msup" transform="translate(10156.1,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="TeXAtom" transform="translate(498,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g><g data-mml-node="mo" transform="translate(298,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1076,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mspace" transform="translate(11818.5,0)"></g><g data-mml-node="msup" transform="translate(11818.5,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mi" transform="translate(498,413) scale(0.707)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g></g><g data-mml-node="mo" transform="translate(12855,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mtext" transform="translate(13910.8,0)"><use data-c="4D" xlink:href="#MJX-1-TEX-N-4D"></use><use data-c="4C" xlink:href="#MJX-1-TEX-N-4C" transform="translate(917,0)"></use><use data-c="50" xlink:href="#MJX-1-TEX-N-50" transform="translate(1542,0)"></use></g><g data-mml-node="mo" transform="translate(16133.8,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mtext" transform="translate(16522.8,0)"><use data-c="4C" xlink:href="#MJX-1-TEX-N-4C"></use><use data-c="4E" xlink:href="#MJX-1-TEX-N-4E" transform="translate(625,0)"></use></g><g data-mml-node="mo" transform="translate(17897.8,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(18286.8,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mi" transform="translate(523,413) scale(0.707)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g></g><g data-mml-node="mo" transform="translate(19070.5,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(19459.5,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(20070.7,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msup" transform="translate(21071,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mi" transform="translate(523,413) scale(0.707)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g></g></g></g></svg></mjx-container>
<p>where <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.419ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3279 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z"></path><path id="MJX-1-TEX-N-53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z"></path><path id="MJX-1-TEX-N-41" d="M255 0Q240 3 140 3Q48 3 39 0H32V46H47Q119 49 139 88Q140 91 192 245T295 553T348 708Q351 716 366 716H376Q396 715 400 709Q402 707 508 390L617 67Q624 54 636 51T687 46H717V0H708Q699 3 581 3Q458 3 437 0H427V46H440Q510 46 510 64Q510 66 486 138L462 209H229L209 150Q189 91 189 85Q189 72 209 59T259 46H264V0H255ZM447 255L345 557L244 256Q244 255 345 255H447Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><use data-c="4D" xlink:href="#MJX-1-TEX-N-4D"></use><use data-c="53" xlink:href="#MJX-1-TEX-N-53" transform="translate(917,0)"></use><use data-c="41" xlink:href="#MJX-1-TEX-N-41" transform="translate(1473,0)"></use></g><g data-mml-node="mo" transform="translate(2223,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(2612,0)"><use data-c="22C5" xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="mo" transform="translate(2890,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container> denotes the multi-head self-attention operation and <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="5.5ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2431 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z"></path><path id="MJX-1-TEX-N-4E" d="M42 46Q74 48 94 56T118 69T128 86V634H124Q114 637 52 637H25V683H232L235 680Q237 679 322 554T493 303L578 178V598Q572 608 568 613T544 627T492 637H475V683H483Q498 680 600 680Q706 680 715 683H724V637H707Q634 633 622 598L621 302V6L614 0H600Q585 0 582 3T481 150T282 443T171 605V345L172 86Q183 50 257 46H274V0H265Q250 3 150 3Q48 3 33 0H25V46H42Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><use data-c="4C" xlink:href="#MJX-1-TEX-N-4C"></use><use data-c="4E" xlink:href="#MJX-1-TEX-N-4E" transform="translate(625,0)"></use></g><g data-mml-node="mo" transform="translate(1375,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(1764,0)"><use data-c="22C5" xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="mo" transform="translate(2042,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container> is the layer norm. Finally, a task-specific head is applied on the CLS token of last layer to acquire the output. Note that these operations are exactly the same across modalities, and thus, a recent line of research tokenizes vision and language separately, and then concatenate them as inputs into a single transformer that performs self-attention across modalities.</p>
<h3 id="why-do-we-need-it">Why Do We Need It?</h3>
<ol type="1">
<li><p><strong>Transfer Learning from Large Dataset</strong></p>
<p>Designing task-specific architecture could be complicated and sometimes impossible since we don't have enough data to train it. By pretraining a transformer on a large and often noisy dataset served for general purpose, we could acquire enough knowledge to easily transfer to a downstream task without changing architecture.</p></li>
<li><p><strong>More Inter-modality Interaction</strong></p>
<p>A fundamental problem of multi-modal learning is modality fusing. Self-attention layers could naturally bridge the two modalities by sharing the same key set between them. This also increases the inter-modality interaction as illustrated from Fig.1.</p></li>
</ol>
<figure>
<img src="/../images/modality_fusing.png" alt="modality_fusing" style="width:100%">
<figcaption align="center">
<b>Fig.1 As illustrated in <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.03334">Kim et al. (2021)</a>, previous modality fusing mostly relies on simple cosine similarity between two embeddings (a&amp;b). By introducing self-attention layers, it increases the interctions between modalities (c&amp;d)</b>
</figcaption>
</figure>
<h3 id="some-pioneering-works">Some Pioneering Works</h3>
<p>Most pioneering works are very similar and only have subtle differences on inputs and pretraining. Moreover, a recent work (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.15124">Bugliarello et al. (2020)</a>) points out that under the same hyperparameter setting, they perform on par with each other. And thus we only show ViLBERT and UNITER as an example in Fig.2 and Fig.3. We also make a summarize in Tab.1 that shows their differences in terms of architecture design, pretraining and downstream tasks according to the original paper. Notice that this is not a comprehensive list.</p>
<figure>
<img src="/../images/vilbert.png" alt="vilbert" style="width:100%">
<figcaption align="center">
<b>Fig.2 ViLBERT, as proposed by <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf">Lu et al. (2019)</a>, is a two stream model that takes RoI features and word embeddings together and encode them separately with two transformers, and finally fuse the features with a multimodal transformer.</b>
</figcaption>
</figure>
<figure>
<img src="/../images/uniter.png" alt="uniter" style="width:100%">
<figcaption align="center">
<b>Fig.2 UNITER, as proposed by <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.11740">Chen et al. (2019)</a>, is a single stream model that takes RoI features and word embeddings together as inputs and encode with a single transformer.</b>
</figcaption>
</figure>
<table style="width:100%;">
<colgroup>
<col style="width: 48%" />
<col style="width: 1%" />
<col style="width: 2%" />
<col style="width: 14%" />
<col style="width: 20%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">model</th>
<th style="text-align: center;">year</th>
<th style="text-align: center;">streams</th>
<th style="text-align: center;">pretrain dataset</th>
<th style="text-align: center;">pretrain objectives</th>
<th style="text-align: center;">downstream task</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf">ViLBERT</a></td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">two</td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;">ITM, MLM, MRM-kl</td>
<td style="text-align: center;">VQA, VCR, REC, IR, zero-shot IR</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://aclanthology.org/D19-1514.pdf">LXMERT</a></td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">two</td>
<td style="text-align: center;">MS COCO, VG, VQA v2.0, GQA, VG-QA</td>
<td style="text-align: center;">ITM, MLM, MRM-reg, MRM-cls</td>
<td style="text-align: center;">VQA, GQA, NLVR</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.03557.pdf">VisualBERT</a></td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">single</td>
<td style="text-align: center;">MS COCO</td>
<td style="text-align: center;">ITM, MLM</td>
<td style="text-align: center;">VQA, VCR, NLVR, Phrase Grounding</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.08530">VLBERT</a></td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">single</td>
<td style="text-align: center;">CC, BooksCorpus, English Wikipedia</td>
<td style="text-align: center;">MLM, MLM (text only), MRM-cls</td>
<td style="text-align: center;">VQA, VCR, REC</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.11740">UNITER</a></td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">single</td>
<td style="text-align: center;">MS COCO, VG, CC, SBU</td>
<td style="text-align: center;">ITM, MLM, MRM-kl, MRM-reg, WRA</td>
<td style="text-align: center;">VQA, VCR, NLVR, REC, IR, TR</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.13198">InterBERT</a></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">two</td>
<td style="text-align: center;">CC, MS COCO, SBU</td>
<td style="text-align: center;">ITM (hard negative), MLM (segment), MRM-cls (high intersection)</td>
<td style="text-align: center;">VCR, IR, zero-shot IR</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://ojs.aaai.org//index.php/AAAI/article/view/7005">Unified VLP</a></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">single</td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;">MLM, Seq2Seq</td>
<td style="text-align: center;">VQA, Caption</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://ojs.aaai.org//index.php/AAAI/article/view/6795">Unicoder-VL</a></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">single</td>
<td style="text-align: center;">CC, SBU</td>
<td style="text-align: center;">ITM, MLM, MRM-cls</td>
<td style="text-align: center;">VCR, IR, TR, zero-shot IR, zero-shot TR</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/hash/49562478de4c54fafd4ec46fdb297de5-Abstract.html">VILLA</a></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">single</td>
<td style="text-align: center;">CC, MS COCO, VG, SBU</td>
<td style="text-align: center;">ITM, MLM, ITM-at, MLM-at</td>
<td style="text-align: center;">VQA, VCR, NLVR, SNLI-VE, REC, IR, TR</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.06165">Oscar</a></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">single</td>
<td style="text-align: center;">CC, MS COCO, SBU, GQA, Flickr30k</td>
<td style="text-align: center;">MLM, Contrastive</td>
<td style="text-align: center;">VQA, GQA, NLVR, IR, TR, Caption, NoCaps</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.html">VinVL</a></td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">single</td>
<td style="text-align: center;">CC, MS COCO, SBU, GQA, Flickr30k, VQA, VG-QA</td>
<td style="text-align: center;">MLM, Contrastive</td>
<td style="text-align: center;">VQA, GQA, NLVR, IR, TR, Caption, NoCaps</td>
</tr>
</tbody>
</table>
<p>[Tab.1]</p>
<p>For inputs, all the models in Tab.1 take RoI features from a pretrained object detector such as Faster-RCNN and project it as image tokens. Text tokens are the same as in BERT. Special tokens and position embeddings are also added in most models.</p>
<p>For architecture design, earilier works like ViLBERT and LXMERT favor two-stream encoder where the vision and language parts are encoded by separate encoders in the beginning, and then fused by a multi-modal encoder. They claim that by encoding them independently, it can preserve the unique characteristics of each modality. However, recent works prove that single stream architecture can also work well, such as VLBERT and UNITER. These models directly concatenate the vision and language tokens as input to a single transformer.</p>
<p>For pretraining datasets choice, four datasets are used the most: Conceptual Captions (CC), MS COCO, Visual Genome (VG) and SBU. Combining them into a larger pretraining dataset will generally lead to better results.</p>
<p>For pretraining objectives, there are usually Image-Text Matching (ITM), Masked Language Modeling (MLM) and Masked Region Modeling (MRM) similar to that in BERT. There is also Word Region Alignment (WRA) as proposed in UNITER. Furthermore, adversarial training is used in VILLA (<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/hash/49562478de4c54fafd4ec46fdb297de5-Abstract.html">Gan et al. (2020)</a>) which boosts the performance.</p>
<p>For downstream tasks, VQA, VCR, Image Retrieval (IR), Text Retrieval (TR), NLVR, Referring Expressiong Comprehension (REC) and Captioning are the most popular ones. Vision-Language Tranformers achieve SOTA results in each of them compared to previous task-specific architectures, which I think mostly thanks to the pretraining-finetuning paradigm and the transformer encoder.</p>
<h3 id="problems-and-how-to-improve-it">Problems and How to Improve It?</h3>
<p>As you might have already noticed, there are several obvious drawbacks such as the use of an object detector. We summarize some more disadvantages in this section and we will also show some examples of how to improve it.</p>
<p>We will first list several <strong>problems</strong> about pioneering works.</p>
<ol type="1">
<li><p><strong>Reliance on Object Detector</strong></p>
<p>On one hand, it could be difficult to have an off-the-shelf object detector sometimes. Lets say we want to build a VL model for medical X-ray reports and there is almost no way we can get an object detector for organs or cancers for free. On the other hand, merely taking object regions as inputs remove all the necessary context information. For instance, as shown in Fig.3, current VL transformers take object bounding boxes as inputs which is the boat, the man and the woman, and there is no way for a model to infer whether the couples are sitting in the boat or on the shore. Another obvious defect is that object detectors are fixed-vocabulary, meaning that they can not handle objects unseen during pretraining, however, we could alwasy encounter long tail object classes in the open ended vision language interaction. And finally, we always have to perform one more inference with an object detection phase at first which makes it inefficient for real-world applications.</p>
<figure>
<p><img src="/../images/lack_global_info.png" alt="lack_global_info" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.2 As shown by <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.03135">Huang et al. (2021)</a>, current vision-language transformers fail to consider the global information in the image.</b></p>
</figcaption>
</figure></li>
<li><p><strong>Learn from Paired Image and Text</strong></p>
<p>These pioneering works always rely on paired image and text as training data, which prevents them from utilizing information from larger amount of independent images and texts. The curated caption datasets usually have simple and structured language which hinders the NLU ability for VL Transformers as shown in <a target="_blank" rel="noopener" href="https://aclanthology.org/2021.emnlp-main.167/">Iki et al. (2021)</a>. Moreover, these annotations could be noisy sometimes which makes pretrained models unreliable.</p></li>
<li><p><strong>Can Multi-task Pretraining Help?</strong></p>
<p>As shown by an early NLP work <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.11504.pdf">Liu et al. (2019)</a>, a simple multi-task training could benefit all the downstream tasks. A similar question could be asked for VL Transformers: Can multi-task learning benefit them?</p></li>
<li><p><strong>How to Finetune?</strong></p>
<p>How do we efficiently utilize these pretrained models for downstream tasks. Do we always need to prepare large amount of data for finetuning? Do we need to finetune all the parameters together?</p></li>
<li><p><strong>Can We Do Image Generation from Them?</strong></p>
<p>Notice that the pioneering works are never applied to an image generation task which makes us curious about its ability on this.</p></li>
<li><p><strong>Can We Compress the Models?</strong></p>
<p>Can we compress the large models into smaller models like we did for DistillBERT?</p></li>
</ol>
<p>We will now present several works dedicated to <strong>solve these problems</strong>.</p>
<ol type="1">
<li><p><strong>Remove Object Detector</strong></p>
<p>An immediate solution for this is to directly input the entire image, however, this is not viable because of the limitation of length for Transformer input. A possible replacement for object RoI features is the final features from CNN models of the image. This is explored in a recent work called Pixel-BERT (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.00849">Huang et al. (2020)</a>) which compresses the input image with ResNet and then flatten CNN features as input tokens for Transformers. In this way, the visual feature extractor is also end-to-end trainable. This work is further extended to SOHO (<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Seeing_Out_of_the_Box_End-to-End_Pre-Training_for_Vision-Language_Representation_CVPR_2021_paper.html">Huang et al. (2021)</a>) which uses visual dictionary embedding to sparse the input information and improve the performance.</p>
<p>Another interesting solution is inspired by the current ViT model. As shown in Fig.3, it breaks the image into multiple patches and then concatenate them with the word embeddings as inputs. ViLT achieves impressive inference speed with lower performance compared to SOTA models. More recently, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.13488">Xue et al. (2021)</a> uses the Swin-T as visual encoder, and it claims that this can improve the ability of extracting long range information in image. <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/hash/505259756244493872b7709a8a01b536-Abstract.html">Li et al. (2021)</a> proposed ALBEF which uses ViT as encoder. It's also worth to note that they align the visual embedding and textual embedding with contrastive loss, which will further promote the inter-modality interaction.</p>
<figure>
<p><img src="/../images/vilt.png" alt="vilt" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.3 ViLT is proposed by <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.03334">Kim et al. (2021)</a>, which uses image patches as visual inputs.</b></p>
</figcaption>
</figure>
<p>Finally, I also want to mention a recent work MDETR (<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kamath_MDETR_-_Modulated_Detection_for_End-to-End_Multi-Modal_Understanding_ICCV_2021_paper.pdf">Kamath et al. (2021)</a>) which is inspired from the hungarian matching loss of DETR. To align with text, they use DETR to propose object queries and then they align each of them with a text span as shown in Fig.4. For example, the 4 pink bounding boxes should be aligned with the phrase "white paws". They use the annotations from the original dataset to supervise this behavior. MDETR shows impressive performance especially on visual grounding tasks, probably because their pretraining objective is similar to this task.</p>
<figure>
<p><img src="/../images/mdetr.png" alt="mdetr" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.4 MDETR is proposed by <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kamath_MDETR_-_Modulated_Detection_for_End-to-End_Multi-Modal_Understanding_ICCV_2021_paper.pdf">Kamath et al. (2021)</a>, which outputs visual queries to align with text spans.</b></p>
</figcaption>
</figure></li>
<li><p><strong>Learn from Unpaired Image and Text</strong></p>
<p>To learn from unpaired image and text in an unsupervised way, <a target="_blank" rel="noopener" href="https://aclanthology.org/2021.naacl-main.420.pdf">Li et al. (2021)</a> propose to train separately from two modalities. On one hand, they use the original MLM loss to train on text alone. On the other hand, they use the concatenation of image tag and image as vision-language pair for Transformer inputs and predict masked tags and masked objects as the pretraining objective. The results shown are promising in that they are almost the same as those trained with paired data.</p>
<figure>
<p><img src="/../images/u_visualbert.png" alt="u_visualbert" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.5 U-VisualBERT is proposed by <a target="_blank" rel="noopener" href="https://aclanthology.org/2021.naacl-main.420.pdf">Li et al. (2021)</a>, they use image tags to help the learning from unpaired image and text.</b></p>
</figcaption>
</figure>
<p>Another interesting idea tries to unify the embeddings from three Transformers: for image, text and multimodal data respectively (<a target="_blank" rel="noopener" href="https://aclanthology.org/2021.acl-long.202/">Li et al. (2021)</a>). It takes advantage from both paired data and unpaired data. And then, they align the embeddings with a contrastive loss as shown in Fig.6. Their experiments show that the proposed model UNIMO can outperform baselines on both multimodal tasks and NLU tasks, which shows that pretraining on unpaired data can help multimodal reasoning and pretraining on vision data can also help the learning of language.</p>
<figure>
<p><img src="/../images/unimo.png" alt="unimo" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.5 UNIMO is proposed by <a target="_blank" rel="noopener" href="https://aclanthology.org/2021.acl-long.202/">Li et al. (2021)</a>, they pretrain on both paired and unpaired data with contrastive loss.</b></p>
</figcaption>
</figure></li>
<li><p><strong>Multi-task Pretraining</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.02315">Lu et al. (2019)</a> extends ViLBERT for multi-task learning and results show that it could benefit independent tasks and at the same time save parameters linearly with respect to number of tasks.</p>
<p>UniT (<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/html/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.html">Hu et al. (2021)</a>) dedicates to solve both single modal and multimodal tasks with a single Transformer model. The tasks include object detection, NLI, and VQA etc.. They encode image and text features separately with two encoders, and then fuse the query feature to form the input for a unified decoder. Finally, a task-specific head is appended on top to solve different tasks independently. In this way, the model parameters are drastically reduced because we share the same model across tasks. It is worth noting that this fashion of unifying modalities into a single model and pretrain the model with multi-task objective is very popular nowadays (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.12993">Likhosherstov et al. (2021)</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.02358">Wang et al. (2021)</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.07074">Li et al. (2021)</a>).</p></li>
<li><p><strong>Few/Zero-shot Finetuning</strong></p>
<p>Prompt-based finetuning has shown to be effective on NLP especially for few/zero-shot scenario, given that there are no extra parameters added to the model (<a target="_blank" rel="noopener" href="https://aclanthology.org/2021.acl-long.295.pdf">Gao et al. (2021)</a>). It is simply a fill-in-the-blank task conditioned on the prompt you give to the model as shown by Fig.6.</p>
<figure>
<p><img src="/../images/prompt.png" alt="prompt" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.6 As shown by <a target="_blank" rel="noopener" href="https://aclanthology.org/2021.acl-long.202/">Gao et al. (2021)</a>, under prompt-based learning a sentiment analysis task can be formulated as filling "positive" or "negative" into the blank. This only requires the MLM head used during pretraining which closes the gap between pretraining task and finetuning task.</b></p>
</figcaption>
</figure>
<p>It is recently shown that prompt-based finetuning could also work for VL Transformers. A recent prompt-based model called prefix-tuning (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00190">Li et al. (2021)</a>) shows that a continuous prompt can also be used as an indicator for the task, where you don't have to interpret the actual meaning of the prompt. Frozen (<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.13884.pdf">Tsimpoukelli et al. (2021)</a>)is ispired by this idea, and they use the image features as a dynamic continuous prompt. The prompts are dynamic in that the input image is changing for each query. And then they only finetune the vision model and fix the language model so that the knowledge learned from pretraining is preserved. As shown in Fig.7, this can be used in VQA as well as few-shot image classification. An interesting application is shown in (b) where the model is asked to answer who invent the iphone given a similar VQA triplet as an example, and the model is able to answer "Steve Jobs" with the knowledge contained in the pretrained language model.</p>
<figure>
<p><img src="/../images/frozen.png" alt="frozen" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.7 As shown by <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.13884.pdf">Tsimpoukelli et al et al. (2021)</a>, Frozen can be used in various few/zero-shot scenarios.</b></p>
</figcaption>
</figure>
<p>Notice that a similar idea is shown by <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v139/cho21a/cho21a.pdf">Cho et al. (2021)</a>. They demonstrated that several multimodal tasks can be combined into a language generation model where given a task query, the model is supposed to give an open-ended answer (Fig.8).</p>
<figure>
<p><img src="/../images/vl_t5.png" alt="vl_t5" style="width:80%"></p>
<figcaption align="center">
<p><b>Fig.8 As shown by <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v139/cho21a/cho21a.pdf">Cho et al et al. (2021)</a>, VL-T5 combines all the multimodal tasks into a generation task.</b></p>
</figcaption>
</figure>
<p>An interesting color-prompt model is also proposed by <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.11797.pdf">Yao et al. (2021)</a>. They use different colored masks to cover the object regions extracted from the image, and they ask the model to answer the corresponding color for the object they are referring to (Fig.9). However, there could be color confusions, for example the model will be confused when the query is asking to find the white cat which is covered by a colored mask.</p>
<figure>
<p><img src="/../images/cpt.png" alt="cpt" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.9 CPT is proposed by <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.11797.pdf">Yao et al et al. (2021)</a>, which uses the colors as a prompt. However, it fails to consider the color confusion when the query contains a color attribute.</b></p>
</figcaption>
</figure></li>
<li><p><strong>Image Generation with VL Transformers</strong></p>
<p>As pointed out by <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.11278">Cho et al. (2020)</a>, when enabled to use grid features (rather than object RoIs) from CNN, LXMERT is still unable to generate meaningful images although it is trained on MRM loss. And thus they extend LXMERT into X-LXMERT which not only generates meaningful images but also preserves a good performance on discrimination tasks. They change the visual inputs to dicrete representations, and they propose to use uniform sampling during pretraining. The results are shown in Fig.10. The generated images are very blurry because of the poor generator. However, we can still see some meaningful structures. In 2021, we might have a better solution by combining <a target="_blank" rel="noopener" href="https://openai.com/blog/clip/">CLIP</a> and GAN.</p>
<figure>
<p><img src="/../images/generated.png" alt="generated" style="width:100%"></p>
<figcaption align="center">
<p><b>Fig.10 Some qualitative examples generated from X-LXMERT (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.11278">Cho et al et al. (2020)</a>)</b></p>
</figcaption>
</figure></li>
<li><p><strong>Compress the Model</strong></p>
<p>In DistillVLM (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.02096">Fang et al. (2021)</a>), they employed the idea of distilling knowledge from a large teacher model to a smaller student model. And they mainly resolve the challenge of object proposal inconsistency between teacher and student.</p></li>
</ol>
<p>I also list papers about improving the VL pretrained Transformers in Tab.2. I will describe their novelties in short. And again, this is not a comprehensive list.</p>
<table>
<colgroup>
<col style="width: 49%" />
<col style="width: 1%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="header">
<th>model</th>
<th>year</th>
<th>novelty</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.02315">12-in-1</a></td>
<td>2019</td>
<td>pretrain a ViLBERT with multi-task loss</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.00849">Pixel-BERT</a></td>
<td>2020</td>
<td>use CNN feature as input</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.06884">DeVLBERT</a></td>
<td>2020</td>
<td>mitigate dataset bias via deconfounding (intervention-based learning)</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.11278">X-LXMERT</a></td>
<td>2020</td>
<td>extend LXMERT for image generation tasks</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/17034">TDEN</a></td>
<td>2021</td>
<td>decoupled network + scheduled sampling to improve especially text generation</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.03334">ViLT</a></td>
<td>2021</td>
<td>w/o CNN, improve efficiency</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/16431">ERNIE-ViL</a></td>
<td>2021</td>
<td>introduce scene graph parsing for fine-grained cross-modal matching</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v139/cho21a/cho21a.pdf">VL-T5</a></td>
<td>2021</td>
<td>unify all multimodal tasks w/ generation task</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.03135">SOHO</a></td>
<td>2021</td>
<td>improve Pixel-BERT by introducing visual dictionary</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kamath_MDETR_-_Modulated_Detection_for_End-to-End_Multi-Modal_Understanding_ICCV_2021_paper.pdf">MDETR</a></td>
<td>2021</td>
<td>inspired from DETR, w/o fixed-vocabulary</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.02096">DistillVLM</a></td>
<td>2021</td>
<td>distill knowledge from large teacher model to smaller student model, first align RoIs</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/html/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.html">UniT</a></td>
<td>2021</td>
<td>unify single-modal and multi-modal with single transformer model</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2021.naacl-main.420.pdf">U-VisualBERT</a></td>
<td>2021</td>
<td>pretrain with unaligned image and text, feed object tag as weak supervision</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2021.acl-long.202/">UNIMO</a></td>
<td>2021</td>
<td>single modal help multi-modal learning, vision helps language learning. use contrastive</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.13884.pdf">Frozen</a></td>
<td>2021</td>
<td>image embedding as prefix for text generating, well-perform on few-shot learning</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.13488">Visual Parsing</a></td>
<td>2021</td>
<td>use Swin-T for image encoding, w/o object detector or visual dictionary, metric for inter-modality flow</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/hash/505259756244493872b7709a8a01b536-Abstract.html">ALBEF</a></td>
<td>2021</td>
<td>use contrastive loss to align vision and language embeddings before feeding into multi-modal encoder, ViT for image encoding, pseudo-labeling for noisy labels</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.11797.pdf">CPT</a></td>
<td>2021</td>
<td>prompt-based tuning for V&amp;L, coloring object regions</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.02358">MoME</a></td>
<td>2021</td>
<td>select modal experts for different modality inputs</td>
</tr>
</tbody>
</table>
<p>[Tab.2]</p>
<h3 id="conclusion-and-discussion">Conclusion and Discussion</h3>
<p>Starting from 2019, VL Transformers are surging multimodal community. Early discussion focus on improving pretraining by increasing size of pretraining data, designing better architectures. And recent works focus on the applicability and quality of model from the angles including removing the reliance on object detectors, learning from unpaired image-text data, more efficient finetuning as well as unifying multiple tasks and modalities into single transformers (only change the task-specific heads or even no change at all).</p>
<blockquote>
<p>"<em>Imagination is more important than knowledge</em> -- Albert Einstein"</p>
</blockquote>
<p>In the future, there might be a model that can be more flexible and lightweight which can take multiple modalities as inputs just like human and doest not need too much computational cost or huamn annotations to reason between modalities.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/08/01/New-Intent-Discovery/" rel="prev" title="New-Intent-Discovery">
      <i class="fa fa-chevron-left"></i> New-Intent-Discovery
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#a-summary-for-vision-language-vl-transformers"><span class="nav-number">1.</span> <span class="nav-text">A Summary for Vision-Language (VL) Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-it"><span class="nav-number">1.0.1.</span> <span class="nav-text">What is it?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-do-we-need-it"><span class="nav-number">1.0.2.</span> <span class="nav-text">Why Do We Need It?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#some-pioneering-works"><span class="nav-number">1.0.3.</span> <span class="nav-text">Some Pioneering Works</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#problems-and-how-to-improve-it"><span class="nav-number">1.0.4.</span> <span class="nav-text">Problems and How to Improve It?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conclusion-and-discussion"><span class="nav-number">1.0.5.</span> <span class="nav-text">Conclusion and Discussion</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yuwei Zhang (张峪玮)"
      src="/images/maomi.jpg">
  <p class="site-author-name" itemprop="name">Yuwei Zhang (张峪玮)</p>
  <div class="site-description" itemprop="description">I am a M.S. student in UCSD. I received B.S. in Physics from Nankai University in China. I currently work on NLP/CV/ML. This blog will record my readings and thoughts from research.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhang-yu-wei" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhang-yu-wei" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/zhangyuwei.work@gmail.com" title="E-Mail → zhangyuwei.work@gmail.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/yuwei-zhang-38a3281a3/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;yuwei-zhang-38a3281a3&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yuwei Zhang (张峪玮)</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script>
  var disqus_config = function() {
    this.page.url = "http://yoursite.com/2021/12/24/VLP/";
    this.page.identifier = "2021/12/24/VLP/";
    this.page.title = "A Summary for Vision-Language Transformers";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://zhang-yu-wei.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
